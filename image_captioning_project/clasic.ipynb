{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y Exploración del Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n",
      "Número de imágenes únicas: 8091\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "captions_file = 'archive/captions.txt'\n",
    "images_folder = 'archive/Images'\n",
    "\n",
    "captions_df = pd.read_csv(captions_file)\n",
    "\n",
    "print(captions_df.head())\n",
    "\n",
    "unique_images = captions_df['image'].unique()\n",
    "print(f\"Número de imágenes únicas: {len(unique_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Todas las imágenes existen!\n"
     ]
    }
   ],
   "source": [
    "missing_images = []\n",
    "for img_name in unique_images:\n",
    "    if not os.path.exists(os.path.join(images_folder, img_name)):\n",
    "        missing_images.append(img_name)\n",
    "\n",
    "if missing_images:\n",
    "    print(\"Imágenes faltantes:\", missing_images)\n",
    "else:\n",
    "    print(\"¡Todas las imágenes existen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesar las descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mauriciotorres/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'child',\n",
       "  'in',\n",
       "  'a',\n",
       "  'pink',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'climbing',\n",
       "  'up',\n",
       "  'a',\n",
       "  'set',\n",
       "  'of',\n",
       "  'stairs',\n",
       "  'in',\n",
       "  'an',\n",
       "  'entry',\n",
       "  'way',\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_list = captions_df['caption'].tolist()\n",
    "tokenized_captions = [word_tokenize(caption.lower().strip()) for caption in captions_list]\n",
    "\n",
    "tokenized_captions[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "vocab = special_tokens + [word for word, count in word_counts.most_common(vocab_size - len(special_tokens))]\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "captions_indices = []\n",
    "for caption in tokenized_captions:\n",
    "    indices = [word_to_idx['<START>']]\n",
    "    for word in caption:\n",
    "        if word in word_to_idx:\n",
    "            indices.append(word_to_idx[word])\n",
    "        else:\n",
    "            indices.append(word_to_idx['<UNK>'])\n",
    "    indices.append(word_to_idx['<END>'])\n",
    "    captions_indices.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo tokenizado: ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.']\n",
      "Ejemplo indexado: [1, 4, 45, 6, 4, 93, 173, 9, 122, 56, 4, 399, 14, 396, 6, 31, 3, 697, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo tokenizado:\", tokenized_captions[0])\n",
    "print(\"Ejemplo indexado:\", captions_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesar las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir las transformaciones necesarias para las imágenes.\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformaciones estándar para ResNet\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar\n",
    "    transforms.ToTensor(),          # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass ImageCaptionDataset(Dataset):\\n    def __init__(self, captions_df, images_folder, transform, word_to_idx, max_length=50):\\n        self.captions_df = captions_df\\n        self.images_folder = images_folder\\n        self.transform = transform\\n        self.word_to_idx = word_to_idx\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.captions_df)\\n\\n    def __getitem__(self, idx):\\n        # Obtener la imagen y su caption\\n        img_name = self.captions_df.iloc[idx]['image']\\n        caption = self.captions_df.iloc[idx]['caption']\\n\\n        # Cargar y transformar la imagen\\n        image = Image.open(os.path.join(self.images_folder, img_name)).convert('RGB')\\n        image = self.transform(image)\\n\\n        # Tokenizar y convertir caption a índices\\n        tokens = word_tokenize(caption.lower().strip())\\n        indices = [self.word_to_idx['<START>']]\\n        indices.extend([self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens])\\n        indices.append(self.word_to_idx['<END>'])\\n\\n        # Padding para que todas las secuencias tengan la misma longitud\\n        if len(indices) < self.max_length:\\n            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\\n        else:\\n            indices = indices[:self.max_length-1] + [self.word_to_idx['<END>']]\\n\\n        return image, torch.tensor(indices)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, captions_df, images_folder, transform, word_to_idx, max_length=50):\n",
    "        self.captions_df = captions_df\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener la imagen y su caption\n",
    "        img_name = self.captions_df.iloc[idx]['image']\n",
    "        caption = self.captions_df.iloc[idx]['caption']\n",
    "\n",
    "        # Cargar y transformar la imagen\n",
    "        image = Image.open(os.path.join(self.images_folder, img_name)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Tokenizar y convertir caption a índices\n",
    "        tokens = word_tokenize(caption.lower().strip())\n",
    "        indices = [self.word_to_idx['<START>']]\n",
    "        indices.extend([self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens])\n",
    "        indices.append(self.word_to_idx['<END>'])\n",
    "\n",
    "        # Padding para que todas las secuencias tengan la misma longitud\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length-1] + [self.word_to_idx['<END>']]\n",
    "\n",
    "        return image, torch.tensor(indices)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch de imágenes: torch.Size([128, 3, 224, 224])\n",
      "Batch de captions: torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "from dataset import ImageCaptionDataset\n",
    "\n",
    "\n",
    "dataset = ImageCaptionDataset(\n",
    "    captions_df=captions_df,\n",
    "    images_folder='archive/Images',\n",
    "    transform=image_transform,\n",
    "    word_to_idx=word_to_idx\n",
    ")\n",
    "\n",
    "# Crear el dataloader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Ajustar según tu CPU\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Verificar un batch\n",
    "for images, captions in dataloader:\n",
    "    print(\"Batch de imágenes:\", images.shape)  # Debería ser [batch_size, 3, 224, 224]\n",
    "    print(\"Batch de captions:\", captions.shape)  # Debería ser [batch_size, max_length]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Extrae caracteristicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 es una red neuronal convolucional profunda con 50 capas que usa bloques de residual \n",
    "# learning (conexiones residuales) para facilitar el entrenamiento de redes muy profundas. Fue \n",
    "# diseñada para mejorar la precisión en tareas de visión por computadora como clasificación, \n",
    "# detección y extracción de características, evitando el problema del vanishing gradient. \n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # Cargar ResNet50 preentrenada\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Eliminar la última capa (fully connected)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # Añadir una capa lineal para ajustar la dimensión\n",
    "        self.linear = nn.Linear(2048, embed_size)  # ResNet50 produce 2048 características\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # Extraer características\n",
    "        features = self.resnet(images)\n",
    "        # Eliminar la dimensión extra\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # Ajustar la dimensión\n",
    "        features = self.linear(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder (genera la descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        # Embedding de las descripciones\n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        # Concatenar características de la imagen con las descripciones\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        \n",
    "        # LSTM\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        \n",
    "        # Capa lineal para predecir la siguiente palabra\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntamos decoder y encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)  # Pasamos embed_size al encoder\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # Extraer características de la imagen\n",
    "        features = self.encoder(images)\n",
    "        \n",
    "        # Generar la descripción\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la salida: torch.Size([128, 51, 5000])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Usar el tamaño del vocabulario que creamos antes\n",
    "\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# 5. Verificar que el modelo funciona\n",
    "# Obtener un batch de prueba\n",
    "for images, captions in dataloader:\n",
    "    # Forward pass\n",
    "    outputs = model(images, captions)\n",
    "    print(\"Shape de la salida:\", outputs.shape)  # Debería ser [batch_size, max_length, vocab_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del modelo:\n",
      "Input images shape: torch.Size([128, 3, 224, 224])\n",
      "Input captions shape: torch.Size([128, 50])\n",
      "Output shape: torch.Size([128, 51, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Verificar el modelo\n",
    "for images, captions in dataloader:\n",
    "    outputs = model(images, captions)\n",
    "    print(\"Dimensiones del modelo:\")\n",
    "    print(f\"Input images shape: {images.shape}\")\n",
    "    print(f\"Input captions shape: {captions.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏         | 4/317 [03:13<4:11:30, 48.21s/it]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Crear el modelo (si no está creado)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# 2. Definir criterio y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Función de entrenamiento\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Poner el modelo en modo entrenamiento\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        # Usar tqdm para mostrar una barra de progreso\n",
    "        for images, captions in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            # Limpiar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, captions)  # [batch_size, seq_length, vocab_size]\n",
    "            \n",
    "            # Preparar los tensores para la pérdida\n",
    "            # Ajustar las dimensiones para que coincidan\n",
    "            outputs = outputs[:, :50, :].contiguous()  # Mantener solo 50 tokens\n",
    "            captions = captions.contiguous()           # Mantener los captions originales\n",
    "            \n",
    "            # Reshape para la función de pérdida\n",
    "            batch_size = outputs.size(0)\n",
    "            seq_length = outputs.size(1)\n",
    "            \n",
    "            # Reshape para la función de pérdida\n",
    "            outputs = outputs.view(batch_size * seq_length, -1)  # [batch_size * seq_length, vocab_size]\n",
    "            captions = captions.view(batch_size * seq_length)    # [batch_size * seq_length]\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(outputs, captions)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actualizar los pesos\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Actualizar la pérdida\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Imprimir la pérdida promedio\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Guardar el modelo cada 5 épocas\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "num_epochs = 10\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar y Ajustar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
