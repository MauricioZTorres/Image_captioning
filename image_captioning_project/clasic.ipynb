{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y Exploración del Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n",
      "Número de imágenes únicas: 8091\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "captions_file = 'archive/captions.txt'\n",
    "images_folder = 'archive/Images'\n",
    "\n",
    "captions_df = pd.read_csv(captions_file)\n",
    "\n",
    "print(captions_df.head())\n",
    "\n",
    "unique_images = captions_df['image'].unique()\n",
    "print(f\"Número de imágenes únicas: {len(unique_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Todas las imágenes existen!\n"
     ]
    }
   ],
   "source": [
    "missing_images = []\n",
    "for img_name in unique_images:\n",
    "    if not os.path.exists(os.path.join(images_folder, img_name)):\n",
    "        missing_images.append(img_name)\n",
    "\n",
    "if missing_images:\n",
    "    print(\"Imágenes faltantes:\", missing_images)\n",
    "else:\n",
    "    print(\"¡Todas las imágenes existen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesar las descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mauriciotorres/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'child',\n",
       "  'in',\n",
       "  'a',\n",
       "  'pink',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'climbing',\n",
       "  'up',\n",
       "  'a',\n",
       "  'set',\n",
       "  'of',\n",
       "  'stairs',\n",
       "  'in',\n",
       "  'an',\n",
       "  'entry',\n",
       "  'way',\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_list = captions_df['caption'].tolist()\n",
    "tokenized_captions = [word_tokenize(caption.lower().strip()) for caption in captions_list]\n",
    "\n",
    "tokenized_captions[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "vocab = special_tokens + [word for word, count in word_counts.most_common(vocab_size - len(special_tokens))]\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "captions_indices = []\n",
    "for caption in tokenized_captions:\n",
    "    indices = [word_to_idx['<START>']]\n",
    "    for word in caption:\n",
    "        if word in word_to_idx:\n",
    "            indices.append(word_to_idx[word])\n",
    "        else:\n",
    "            indices.append(word_to_idx['<UNK>'])\n",
    "    indices.append(word_to_idx['<END>'])\n",
    "    captions_indices.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo tokenizado: ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.']\n",
      "Ejemplo indexado: [1, 4, 45, 6, 4, 93, 173, 9, 122, 56, 4, 399, 14, 396, 6, 31, 3, 697, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo tokenizado:\", tokenized_captions[0])\n",
    "print(\"Ejemplo indexado:\", captions_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesar las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir las transformaciones necesarias para las imágenes.\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformaciones estándar para ResNet\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar\n",
    "    transforms.ToTensor(),          # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass ImageCaptionDataset(Dataset):\\n    def __init__(self, captions_df, images_folder, transform, word_to_idx, max_length=50):\\n        self.captions_df = captions_df\\n        self.images_folder = images_folder\\n        self.transform = transform\\n        self.word_to_idx = word_to_idx\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.captions_df)\\n\\n    def __getitem__(self, idx):\\n        # Obtener la imagen y su caption\\n        img_name = self.captions_df.iloc[idx]['image']\\n        caption = self.captions_df.iloc[idx]['caption']\\n\\n        # Cargar y transformar la imagen\\n        image = Image.open(os.path.join(self.images_folder, img_name)).convert('RGB')\\n        image = self.transform(image)\\n\\n        # Tokenizar y convertir caption a índices\\n        tokens = word_tokenize(caption.lower().strip())\\n        indices = [self.word_to_idx['<START>']]\\n        indices.extend([self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens])\\n        indices.append(self.word_to_idx['<END>'])\\n\\n        # Padding para que todas las secuencias tengan la misma longitud\\n        if len(indices) < self.max_length:\\n            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\\n        else:\\n            indices = indices[:self.max_length-1] + [self.word_to_idx['<END>']]\\n\\n        return image, torch.tensor(indices)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, captions_df, images_folder, transform, word_to_idx, max_length=50):\n",
    "        self.captions_df = captions_df\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener la imagen y su caption\n",
    "        img_name = self.captions_df.iloc[idx]['image']\n",
    "        caption = self.captions_df.iloc[idx]['caption']\n",
    "\n",
    "        # Cargar y transformar la imagen\n",
    "        image = Image.open(os.path.join(self.images_folder, img_name)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Tokenizar y convertir caption a índices\n",
    "        tokens = word_tokenize(caption.lower().strip())\n",
    "        indices = [self.word_to_idx['<START>']]\n",
    "        indices.extend([self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens])\n",
    "        indices.append(self.word_to_idx['<END>'])\n",
    "\n",
    "        # Padding para que todas las secuencias tengan la misma longitud\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length-1] + [self.word_to_idx['<END>']]\n",
    "\n",
    "        return image, torch.tensor(indices)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch de imágenes: torch.Size([128, 3, 224, 224])\n",
      "Batch de captions: torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "from dataset import ImageCaptionDataset\n",
    "\n",
    "\n",
    "dataset = ImageCaptionDataset(\n",
    "    captions_df=captions_df,\n",
    "    images_folder='archive/Images',\n",
    "    transform=image_transform,\n",
    "    word_to_idx=word_to_idx\n",
    ")\n",
    "\n",
    "# Crear el dataloader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Ajustar según tu CPU\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Verificar un batch\n",
    "for images, captions in dataloader:\n",
    "    print(\"Batch de imágenes:\", images.shape)  # Debería ser [batch_size, 3, 224, 224]\n",
    "    print(\"Batch de captions:\", captions.shape)  # Debería ser [batch_size, max_length]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Extrae caracteristicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 es una red neuronal convolucional profunda con 50 capas que usa bloques de residual \n",
    "# learning (conexiones residuales) para facilitar el entrenamiento de redes muy profundas. Fue \n",
    "# diseñada para mejorar la precisión en tareas de visión por computadora como clasificación, \n",
    "# detección y extracción de características, evitando el problema del vanishing gradient. \n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # Cargar ResNet50 preentrenada\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Eliminar la última capa (fully connected)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # Añadir una capa lineal para ajustar la dimensión\n",
    "        self.linear = nn.Linear(2048, embed_size)  # ResNet50 produce 2048 características\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # Extraer características\n",
    "        features = self.resnet(images)\n",
    "        # Eliminar la dimensión extra\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # Ajustar la dimensión\n",
    "        features = self.linear(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder (genera la descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        # Embedding de las descripciones\n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        # Concatenar características de la imagen con las descripciones\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        \n",
    "        # LSTM\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        \n",
    "        # Capa lineal para predecir la siguiente palabra\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntamos decoder y encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)  # Pasamos embed_size al encoder\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # Extraer características de la imagen\n",
    "        features = self.encoder(images)\n",
    "        \n",
    "        # Generar la descripción\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mauriciotorres/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la salida: torch.Size([128, 51, 5000])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Usar el tamaño del vocabulario que creamos antes\n",
    "\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# 5. Verificar que el modelo funciona\n",
    "# Obtener un batch de prueba\n",
    "for images, captions in dataloader:\n",
    "    # Forward pass\n",
    "    outputs = model(images, captions)\n",
    "    print(\"Shape de la salida:\", outputs.shape)  # Debería ser [batch_size, max_length, vocab_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del modelo:\n",
      "Input images shape: torch.Size([128, 3, 224, 224])\n",
      "Input captions shape: torch.Size([128, 50])\n",
      "Output shape: torch.Size([128, 51, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Verificar el modelo\n",
    "for images, captions in dataloader:\n",
    "    outputs = model(images, captions)\n",
    "    print(\"Dimensiones del modelo:\")\n",
    "    print(f\"Input images shape: {images.shape}\")\n",
    "    print(f\"Input captions shape: {captions.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏         | 4/317 [04:02<5:16:49, 60.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 4. Entrenar el modelo\u001b[39;00m\n\u001b[32m     62\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     42\u001b[39m loss = criterion(outputs, captions)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Actualizar los pesos\u001b[39;00m\n\u001b[32m     48\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_IAP/image_captioning_project/venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Crear el modelo (si no está creado)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# 2. Definir criterio y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Función de entrenamiento\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Poner el modelo en modo entrenamiento\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        # Usar tqdm para mostrar una barra de progreso\n",
    "        for images, captions in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            # Limpiar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, captions)  # [batch_size, seq_length, vocab_size]\n",
    "            \n",
    "            # Preparar los tensores para la pérdida\n",
    "            # Ajustar las dimensiones para que coincidan\n",
    "            outputs = outputs[:, :50, :].contiguous()  # Mantener solo 50 tokens\n",
    "            captions = captions.contiguous()           # Mantener los captions originales\n",
    "            \n",
    "            # Reshape para la función de pérdida\n",
    "            batch_size = outputs.size(0)\n",
    "            seq_length = outputs.size(1)\n",
    "            \n",
    "            # Reshape para la función de pérdida\n",
    "            outputs = outputs.view(batch_size * seq_length, -1)  # [batch_size * seq_length, vocab_size]\n",
    "            captions = captions.view(batch_size * seq_length)    # [batch_size * seq_length]\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(outputs, captions)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actualizar los pesos\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Actualizar la pérdida\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Imprimir la pérdida promedio\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Guardar el modelo cada 5 épocas\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "num_epochs = 10\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar y Ajustar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, idx_to_word):\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            # Generar predicciones\n",
    "            outputs = model(images, captions)\n",
    "            \n",
    "            # Convertir predicciones a palabras\n",
    "            predicted_indices = outputs.argmax(dim=-1)\n",
    "            \n",
    "            # Mostrar algunos ejemplos\n",
    "            for i in range(min(5, len(images))):\n",
    "                print(\"\\nImagen\", i+1)\n",
    "                print(\"Predicción:\", ' '.join([idx_to_word[idx.item()] for idx in predicted_indices[i]]))\n",
    "                print(\"Real:\", ' '.join([idx_to_word[idx.item()] for idx in captions[i]]))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_path, transform, word_to_idx, idx_to_word, max_length=50):\n",
    "    # Cargar y transformar la imagen\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Inicializar la secuencia con el token START\n",
    "    start_token = word_to_idx['<START>']\n",
    "    sequence = [start_token]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Extraer características de la imagen\n",
    "        features = model.encoder(image)\n",
    "        \n",
    "        # Generar la descripción token por token\n",
    "        for _ in range(max_length):\n",
    "            # Convertir la secuencia actual a tensor\n",
    "            sequence_tensor = torch.tensor(sequence).unsqueeze(0)\n",
    "            \n",
    "            # Obtener la siguiente predicción\n",
    "            output = model.decoder(features, sequence_tensor)\n",
    "            next_token = output.argmax(dim=-1)[0, -1].item()\n",
    "            \n",
    "            # Agregar el token a la secuencia\n",
    "            sequence.append(next_token)\n",
    "            \n",
    "            # Detener si generamos el token END\n",
    "            if next_token == word_to_idx['<END>']:\n",
    "                break\n",
    "    \n",
    "    # Convertir la secuencia a palabras\n",
    "    caption = ' '.join([idx_to_word[idx] for idx in sequence[1:-1]])  # Excluir START y END\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(image_path, caption):\n",
    "    # Mostrar la imagen y su descripción\n",
    "    image = Image.open(image_path)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.title(caption)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_bleu_score(model, dataloader, word_to_idx, idx_to_word):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            outputs = model(images, captions)\n",
    "            predicted_indices = outputs.argmax(dim=-1)\n",
    "            \n",
    "            # Convertir predicciones a palabras\n",
    "            for i in range(len(images)):\n",
    "                pred = [idx_to_word[idx.item()] for idx in predicted_indices[i]]\n",
    "                ref = [idx_to_word[idx.item()] for idx in captions[i]]\n",
    "                \n",
    "                hypotheses.append(pred)\n",
    "                references.append([ref])  # BLEU espera una lista de referencias\n",
    "    \n",
    "    # Calcular BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def caption_image(image):\n",
    "    # Generar descripción\n",
    "    caption = generate_caption(model, image, image_transform, word_to_idx, idx_to_word)\n",
    "    return caption\n",
    "\n",
    "# Crear interfaz\n",
    "iface = gr.Interface(\n",
    "    fn=caption_image,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Generador de Descripciones de Imágenes\"\n",
    ")\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
